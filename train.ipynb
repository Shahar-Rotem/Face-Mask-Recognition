{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import mobilenet_v2\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import time\n",
    "from glob import glob\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import *\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceMaskDataset(Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "                 root_dir,\n",
    "                 have_label=True,\n",
    "                 transform=transforms.Compose([transforms.RandomResizedCrop(224),\n",
    "                                               transforms.ToTensor(),\n",
    "                                               transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                                    [0.229, 0.224, 0.225])])):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images.\n",
    "            have_label (boolean): Flag for images having the labels in their names.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a batch.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.have_label = have_label\n",
    "        self.transform = transform\n",
    "        self.images_paths = glob(f'{self.root_dir}/*.jpg')\n",
    "        self.labels = None\n",
    "        if have_label:\n",
    "            self.labels = torch.LongTensor([int(image_path.split('.jpg')[0][-1]) for image_path in self.images_paths])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.images_paths[idx]\n",
    "        image = Image.open(image_path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)       \n",
    "        if self.have_label:\n",
    "            label = self.labels[idx]\n",
    "            item = (image, label)\n",
    "        else:\n",
    "            item = image\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, eval_dataloaders, datasets_sizes, criterion, optimizer, device='cuda:0', num_epochs=20, print_epoch=1, scheduler=None, data_types=['train', 'test'], save_model=False):\n",
    "    start_time = time.time()\n",
    "    best_f1 = 0.0\n",
    "    best_model = deepcopy(model.state_dict())\n",
    "    \n",
    "    for epoch in range(0, num_epochs + 1):\n",
    "        start_epoch = time.time()\n",
    "        print(\"Epoch [{}/{}]\".format(str(epoch).zfill(len(str(num_epochs))), num_epochs))\n",
    "        \n",
    "        if epoch != 0:\n",
    "            model.train()\n",
    "\n",
    "            for inputs, labels in train_dataloader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(True):\n",
    "                    outputs = model(inputs)\n",
    "                    \n",
    "                    \n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "           \n",
    "        model.eval()\n",
    "        \n",
    "        for data_type in data_types:\n",
    "            epoch_loss = 0.0\n",
    "            \n",
    "            y_true = np.array([])\n",
    "            y_pred = np.array([])\n",
    "            y_score = np.array([])\n",
    "        \n",
    "            for inputs, labels in eval_dataloaders[data_type]:\n",
    "                y_true = np.append(y_true, labels.numpy())\n",
    "                \n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                                \n",
    "                with torch.set_grad_enabled(False):\n",
    "                    outputs = model(inputs)\n",
    "            \n",
    "                    loss = criterion(outputs, labels)\n",
    "                \n",
    "                    scores = F.softmax(outputs, 1)[:,1]\n",
    "                    y_score = np.append(y_score, scores.to('cpu').numpy())\n",
    "                    \n",
    "                    _, pred = torch.max(outputs, 1)\n",
    "                    y_pred = np.append(y_pred, pred.to('cpu').numpy())\n",
    "                        \n",
    "                epoch_loss += loss.item() * inputs.size(0)\n",
    "                \n",
    "            epoch_loss = epoch_loss / datasets_sizes[data_type]\n",
    "            epoch_acc = accuracy_score(y_true, y_pred) * 100\n",
    "            epoch_p = precision_score(y_true, y_pred, zero_division=0) * 100\n",
    "            epoch_r = recall_score(y_true, y_pred, zero_division=0) * 100\n",
    "            epoch_f1 = f1_score(y_true, y_pred, average='binary', zero_division=0) * 100\n",
    "            epoch_roc_auc = roc_auc_score(y_true, y_score) * 100\n",
    "            \n",
    "            print('{} Loss: {:.4f} F1: {:2.2f} Precision: {:2.2f} Recall: {:2.2f} Accuracy: {:2.2f} ROC-AUC: {:2.2f}'.format(\n",
    "                data_type.ljust(5), epoch_loss, epoch_f1, epoch_p, epoch_r, epoch_acc, epoch_roc_auc))\n",
    "            \n",
    "            if data_type == 'test' and epoch_f1 > best_f1:\n",
    "                best_f1 = epoch_f1\n",
    "                best_model = deepcopy(model.state_dict())\n",
    "        \n",
    "        epoch_elapsed = time.time() - start_epoch\n",
    "        print('Epoch {} took {}m {:.0f}s'.format(\n",
    "        epoch, int(epoch_elapsed // 60), epoch_elapsed % 60))\n",
    "        \n",
    "        print('-' * 90)\n",
    "        \n",
    "    time_elapsed = time.time() - start_time\n",
    "    print('Training complete in {}m {:.0f}s'.format(\n",
    "    int(time_elapsed // 60), time_elapsed % 60))\n",
    "    \n",
    "    print('Best test F1: {:2f}'.format(best_f1))\n",
    "    if save_model:\n",
    "        torch.save(best_model, 'model_{:2f}.pkl'.format(best_f1))\n",
    "    model.load_state_dict(best_model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir='/StudentData/hw2_data'\n",
    "num_classes = 2\n",
    "phases = ['train', 'eval']\n",
    "data_types = ['train', 'test']\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train' : transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]), \n",
    "    'eval' : transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "train_dataset = FaceMaskDataset(root_dir=f'{root_dir}/train', have_label=True, transform=data_transforms['train'])\n",
    "eval_datasets = {data_type : FaceMaskDataset(root_dir=f'{root_dir}/{data_type}', have_label=True, transform=data_transforms['eval']) for data_type in data_types}\n",
    "\n",
    "datasets_sizes = {data_type : len(eval_datasets[data_type]) for data_type in data_types}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = {\n",
    "    'train' : 128,\n",
    "    'eval' : 64\n",
    "}\n",
    "\n",
    "shuffles = {\n",
    "    'train' : True,\n",
    "    'eval' : False\n",
    "}\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_sizes['train'], shuffles['train'])\n",
    "eval_dataloaders = {data_type : DataLoader(eval_datasets[data_type], batch_sizes['eval'], shuffles['eval']) for data_type in data_types}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_bn(inp, oup, stride, conv_layer=nn.Conv2d, norm_layer=nn.BatchNorm2d, nlin_layer=nn.ReLU):\n",
    "    return nn.Sequential(\n",
    "        conv_layer(inp, oup, 3, stride, 1, bias=False),\n",
    "        norm_layer(oup),\n",
    "        nlin_layer(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "def conv_1x1_bn(inp, oup, conv_layer=nn.Conv2d, norm_layer=nn.BatchNorm2d, nlin_layer=nn.ReLU):\n",
    "    return nn.Sequential(\n",
    "        conv_layer(inp, oup, 1, 1, 0, bias=False),\n",
    "        norm_layer(oup),\n",
    "        nlin_layer(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "class Hswish(nn.Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(Hswish, self).__init__()\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * F.relu6(x + 3., inplace=self.inplace) / 6.\n",
    "\n",
    "\n",
    "class Hsigmoid(nn.Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(Hsigmoid, self).__init__()\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.relu6(x + 3., inplace=self.inplace) / 6.\n",
    "\n",
    "\n",
    "class SEModule(nn.Module):\n",
    "    def __init__(self, channel, reduction=4):\n",
    "        super(SEModule, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            Hsigmoid()\n",
    "            # nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "\n",
    "class Identity(nn.Module):\n",
    "    def __init__(self, channel):\n",
    "        super(Identity, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "\n",
    "def make_divisible(x, divisible_by=8):\n",
    "    import numpy as np\n",
    "    return int(np.ceil(x * 1. / divisible_by) * divisible_by)\n",
    "\n",
    "\n",
    "class MobileBottleneck(nn.Module):\n",
    "    def __init__(self, inp, oup, kernel, stride, exp, se=False, nl='RE'):\n",
    "        super(MobileBottleneck, self).__init__()\n",
    "        assert stride in [1, 2]\n",
    "        assert kernel in [3, 5]\n",
    "        padding = (kernel - 1) // 2\n",
    "        self.use_res_connect = stride == 1 and inp == oup\n",
    "\n",
    "        conv_layer = nn.Conv2d\n",
    "        norm_layer = nn.BatchNorm2d\n",
    "        if nl == 'RE':\n",
    "            nlin_layer = nn.ReLU # or ReLU6\n",
    "        elif nl == 'HS':\n",
    "            nlin_layer = Hswish\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        if se:\n",
    "            SELayer = SEModule\n",
    "        else:\n",
    "            SELayer = Identity\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            # pw\n",
    "            conv_layer(inp, exp, 1, 1, 0, bias=False),\n",
    "            norm_layer(exp),\n",
    "            nlin_layer(inplace=True),\n",
    "            # dw\n",
    "            conv_layer(exp, exp, kernel, stride, padding, groups=exp, bias=False),\n",
    "            norm_layer(exp),\n",
    "            SELayer(exp),\n",
    "            nlin_layer(inplace=True),\n",
    "            # pw-linear\n",
    "            conv_layer(exp, oup, 1, 1, 0, bias=False),\n",
    "            norm_layer(oup),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_res_connect:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class MobileNetV3(nn.Module):\n",
    "    def __init__(self, n_class=1000, input_size=224, dropout=0.8, mode='small', width_mult=1.0):\n",
    "        super(MobileNetV3, self).__init__()\n",
    "        input_channel = 16\n",
    "        last_channel = 1280\n",
    "        if mode == 'large':\n",
    "            # refer to Table 1 in paper\n",
    "            mobile_setting = [\n",
    "                # k, exp, c,  se,     nl,  s,\n",
    "                [3, 16,  16,  False, 'RE', 1],\n",
    "                [3, 64,  24,  False, 'RE', 2],\n",
    "                [3, 72,  24,  False, 'RE', 1],\n",
    "                [5, 72,  40,  True,  'RE', 2],\n",
    "                [5, 120, 40,  True,  'RE', 1],\n",
    "                [5, 120, 40,  True,  'RE', 1],\n",
    "                [3, 240, 80,  False, 'HS', 2],\n",
    "                [3, 200, 80,  False, 'HS', 1],\n",
    "                [3, 184, 80,  False, 'HS', 1],\n",
    "                [3, 184, 80,  False, 'HS', 1],\n",
    "                [3, 480, 112, True,  'HS', 1],\n",
    "                [3, 672, 112, True,  'HS', 1],\n",
    "                [5, 672, 160, True,  'HS', 2],\n",
    "                [5, 960, 160, True,  'HS', 1],\n",
    "                [5, 960, 160, True,  'HS', 1],\n",
    "            ]\n",
    "        elif mode == 'small':\n",
    "            # refer to Table 2 in paper\n",
    "            mobile_setting = [\n",
    "                # k, exp, c,  se,     nl,  s,\n",
    "                [3, 16,  16,  True,  'RE', 2],\n",
    "                [3, 72,  24,  False, 'RE', 2],\n",
    "                [3, 88,  24,  False, 'RE', 1],\n",
    "                [5, 96,  40,  True,  'HS', 2],\n",
    "                [5, 240, 40,  True,  'HS', 1],\n",
    "                [5, 240, 40,  True,  'HS', 1],\n",
    "                [5, 120, 48,  True,  'HS', 1],\n",
    "                [5, 144, 48,  True,  'HS', 1],\n",
    "                [5, 288, 96,  True,  'HS', 2],\n",
    "                [5, 576, 96,  True,  'HS', 1],\n",
    "                [5, 576, 96,  True,  'HS', 1],\n",
    "            ]\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # building first layer\n",
    "        assert input_size % 32 == 0\n",
    "        last_channel = make_divisible(last_channel * width_mult) if width_mult > 1.0 else last_channel\n",
    "        self.features = [conv_bn(3, input_channel, 2, nlin_layer=Hswish)]\n",
    "        self.classifier = []\n",
    "\n",
    "        # building mobile blocks\n",
    "        for k, exp, c, se, nl, s in mobile_setting:\n",
    "            output_channel = make_divisible(c * width_mult)\n",
    "            exp_channel = make_divisible(exp * width_mult)\n",
    "            self.features.append(MobileBottleneck(input_channel, output_channel, k, s, exp_channel, se, nl))\n",
    "            input_channel = output_channel\n",
    "\n",
    "        # building last several layers\n",
    "        if mode == 'large':\n",
    "            last_conv = make_divisible(960 * width_mult)\n",
    "            self.features.append(conv_1x1_bn(input_channel, last_conv, nlin_layer=Hswish))\n",
    "            self.features.append(nn.AdaptiveAvgPool2d(1))\n",
    "            self.features.append(nn.Conv2d(last_conv, last_channel, 1, 1, 0))\n",
    "            self.features.append(Hswish(inplace=True))\n",
    "        elif mode == 'small':\n",
    "            last_conv = make_divisible(576 * width_mult)\n",
    "            self.features.append(conv_1x1_bn(input_channel, last_conv, nlin_layer=Hswish))\n",
    "            # self.features.append(SEModule(last_conv))  # refer to paper Table2, but I think this is a mistake\n",
    "            self.features.append(nn.AdaptiveAvgPool2d(1))\n",
    "            self.features.append(nn.Conv2d(last_conv, last_channel, 1, 1, 0))\n",
    "            self.features.append(Hswish(inplace=True))\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # make it nn.Sequential\n",
    "        self.features = nn.Sequential(*self.features)\n",
    "\n",
    "        # building classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=dropout),    # refer to paper section 6\n",
    "            nn.Linear(last_channel, n_class),\n",
    "        )\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.mean(3).mean(2)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        # weight initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = mobilenet_v2(pretrained=False)\n",
    "# model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "\n",
    "model = MobileNetV3(n_class=2, input_size=224, dropout=0.0, mode='large', width_mult=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Number of Parameters: {:.2f}M'.format(sum(param.numel() for param in model.parameters()) / 1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate *= gamma every (step_size) epochs\n",
    "learning_rate = 1e-3\n",
    "step_size = 5\n",
    "gamma = 1e-1\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = train_model(model, train_dataloader, eval_dataloaders, datasets_sizes, criterion, optimizer, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda-test_env",
   "language": "python",
   "name": "conda-env-.conda-test_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
